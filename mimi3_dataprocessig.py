# -*- coding: utf-8 -*-
"""MIMI3_Dataprocessig.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jVnIQ7pHoEZxpuG7-1iVMVTMO3FDh8WE
"""

# ==============================================================================
# FINAL TRAINING NOTEBOOK (with CHARTEVENTS and LIME)
# ==============================================================================

# 1. Install Libraries
!pip install scikit-learn pandas joblib lime -q
print("✅ Libraries installed.")

# 2. Import Libraries
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
import joblib
import lime
import lime.lime_tabular
import numpy as np
from google.colab import drive

# 3. Mount Drive and Load Data
drive.mount('/content/drive')
base_path = '/content/drive/MyDrive/CapstoneProject/' # Added '/' here

# Load all EIGHT MIMIC-III CSV files
# Using low_memory=False for better type inference on large files
df_admissions = pd.read_csv(base_path + 'ADMISSIONS.csv', low_memory=False)
df_patients = pd.read_csv(base_path + 'PATIENTS.csv', low_memory=False)
df_diagnoses = pd.read_csv(base_path + 'DIAGNOSES_ICD.csv', low_memory=False)
df_labitems = pd.read_csv(base_path + 'D_LABITEMS.csv', low_memory=False)
df_labevents = pd.read_csv(base_path + 'LABEVENTS.csv', low_memory=False)
df_items = pd.read_csv(base_path + 'D_ITEMS.csv', low_memory=False)
df_chartevents = pd.read_csv(base_path + 'CHARTEVENTS.csv', low_memory=False)
df_prescriptions = pd.read_csv(base_path + 'PRESCRIPTIONS.csv', low_memory=False) # Not used in this model, but loaded
print("✅ All eight MIMIC-III data tables loaded successfully!")


# 4. Create Heart Failure Patient Cohort & Calculate Age
hf_icd_codes = df_diagnoses[df_diagnoses['icd9_code'].str.startswith('428', na=False)]
hf_admissions_ids = hf_icd_codes['hadm_id'].unique()
df_hf_admissions = df_admissions[df_admissions['hadm_id'].isin(hf_admissions_ids)]
df_merged = pd.merge(df_hf_admissions, df_patients, on='subject_id', how='inner')

df_merged['admittime'] = pd.to_datetime(df_merged['admittime'])
df_merged['dob'] = pd.to_datetime(df_merged['dob'])

def calculate_age(row):
    try:
        return (row['admittime'].to_pydatetime() - row['dob'].to_pydatetime()).days / 365.25
    except: return np.nan
df_merged['age'] = df_merged.apply(calculate_age, axis=1)
df_merged.loc[df_merged['age'] > 89, 'age'] = 90
df_base = df_merged[['subject_id', 'hadm_id', 'age', 'gender', 'hospital_expire_flag']].drop_duplicates()
print("✅ Base table of heart failure patients created.")


# 5. Process and Add Lab Features
key_lab_labels = ['Creatinine', 'Potassium', 'Sodium', 'Urea Nitrogen', 'Bicarbonate']
key_lab_ids = df_labitems[df_labitems['label'].isin(key_lab_labels)]['itemid']
df_key_labs = df_labevents[df_labevents['itemid'].isin(key_lab_ids)]
df_labs_avg = df_key_labs.groupby(['hadm_id', 'itemid'])['valuenum'].mean().reset_index()
df_labs_avg = pd.merge(df_labs_avg, df_labitems[['itemid', 'label']], on='itemid', how='inner')
df_labs_pivot = df_labs_avg.pivot_table(index='hadm_id', columns='label', values='valuenum', aggfunc='mean')
df_labs_pivot.columns = [col.replace(' ', '_') + '_avg' for col in df_labs_pivot.columns]
df_final = pd.merge(df_base, df_labs_pivot, on='hadm_id', how='left')
print("✅ Lab results processed and added.")


# 6. Process and Add Vitals/BMI from CHARTEVENTS (NEW STEP)
bp_systolic_id = 51 # From D_ITEMS, common ID for non-invasive systolic BP
bp_diastolic_id = 8368 # From D_ITEMS, common ID for non-invasive diastolic BP
weight_id = 763 # From D_ITEMS, common ID for admission weight (kg)
height_id = 920 # From D_ITEMS, common ID for admission height (cm)


key_chart_ids = [bp_systolic_id, bp_diastolic_id, weight_id, height_id]
df_key_charts = df_chartevents[df_chartevents['itemid'].isin(key_chart_ids)]
df_charts_avg = df_key_charts.groupby(['hadm_id', 'itemid'])['valuenum'].mean().reset_index()
df_charts_avg['label'] = df_charts_avg['itemid'].map({bp_systolic_id: 'Systolic_BP', bp_diastolic_id: 'Diastolic_BP', weight_id: 'Weight_kg', height_id: 'Height_cm'})
df_charts_pivot = df_charts_avg.pivot_table(index='hadm_id', columns='label', values='valuenum', aggfunc='mean')

df_final = pd.merge(df_final, df_charts_pivot, on='hadm_id', how='left')
# Calculate BMI = kg / (m^2)
df_final['BMI'] = df_final['Weight_kg'] / ((df_final['Height_cm'] / 100) ** 2)
print("✅ Vitals (BP) and BMI processed and added.")


# 7. Final Cleanup and Model Training
# Select final features
features = ['age', 'gender', 'Bicarbonate_avg', 'Creatinine_avg', 'Potassium_avg', 'Sodium_avg', 'Urea_Nitrogen_avg', 'Systolic_BP', 'Diastolic_BP', 'BMI']
df_final = df_final.dropna(subset=features) # Drop rows where any of our key features are missing
X = df_final[features]
y = df_final['hospital_expire_flag']
X['gender'] = df_final['gender'].apply(lambda s: 1 if s == 'F' else 0)

model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')
model.fit(X, y)

# Create the LIME Explainer
explainer = lime.lime_tabular.LimeTabularExplainer(
    training_data=X.values,
    feature_names=X.columns.tolist(),
    class_names=['Survived', 'Expired'],
    mode='classification'
)
print("✅ Final model and LIME explainer created.")

# 8. Save the Final Model and Explainer
joblib.dump(model, '/content/drive/MyDrive/CapstoneProject/mimic_model.joblib')
# Save feature names separately
joblib.dump(X.columns.tolist(), '/content/drive/MyDrive/CapstoneProject/mimic_feature_names.joblib')

# Note: LIME explainer itself is often not directly pickleable due to internal lambda functions.
# To use the explainer later, you'll need to reload the data and feature names,
# and then reconstruct the explainer using the training data and feature names.
print("\n✅ SUCCESS: Your new, enhanced model and LIME feature names have been saved to Google Drive!")